{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from timetransfusion import TTFModel, create_inout_sequences, get_batch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "CONTEXT_LENGTH = 14 # Number of past datapoints used for prediction\n",
    "PREDICTION_LENGTH = 7 # How many timesteps into the future we want the model to predict\n",
    "BATCH_SIZE = 32 # Sizes of training and testing batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from rain dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_excel('../datasets/rain.xlsx', index_col=2) \n",
    "\n",
    "bergen = data[data[\"Navn\"]==\"Bergen - Florida Uib\"]\n",
    "oslo = data[data[\"Navn\"]==\"Oslo - Blindern\"]\n",
    "\n",
    "rain = torch.FloatTensor(list(bergen.iloc[:,2]))\n",
    "\n",
    "# Split data into a training a testing dataset\n",
    "train_data = rain[:1500]\n",
    "test_data = rain[1500:]\n",
    "\n",
    "# Convert time series from form [x_0,x_1,...,x_n] to form [(context_0,target_0),...,(context_n,target_n)]\n",
    "train_data = create_inout_sequences(train_data, CONTEXT_LENGTH, PREDICTION_LENGTH)\n",
    "test_data = create_inout_sequences(test_data, CONTEXT_LENGTH, PREDICTION_LENGTH)\n",
    "\n",
    "# Divide data into batches\n",
    "#train_data = batchify(train_data,BATCH_SIZE,device)\n",
    "#test_data = batchify(test_data,BATCH_SIZE,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TTFModel(\n",
    "    d_model = 1, \n",
    "    nhead = 3, \n",
    "    d_hid = 200,\n",
    "    nlayers = 2,\n",
    "    encoding_length=8,\n",
    "    dropout = 0.1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "lr = 0.005 \n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 40 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "calculate_loss_over_all_values = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(len(train_data) // BATCH_SIZE):\n",
    "        data, targets = get_batch(train_data, BATCH_SIZE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data,targets)        \n",
    "\n",
    "        if calculate_loss_over_all_values:\n",
    "            loss = criterion(output, targets)\n",
    "        else:\n",
    "            loss = criterion(output[-PREDICTION_LENGTH:], targets[-PREDICTION_LENGTH:])\n",
    "    \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / BATCH_SIZE / 5)\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} |'.format(\n",
    "                    epoch, i, len(train_data) // BATCH_SIZE, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss))#, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_data) // BATCH_SIZE):\n",
    "            source, targets = get_batch(data_source,BATCH_SIZE)\n",
    "            output = eval_model(source,targets)            \n",
    "            if calculate_loss_over_all_values:\n",
    "                total_loss += len(source[0])* criterion(output, targets).cpu().item()\n",
    "            else:                                \n",
    "                total_loss += len(source[0])* criterion(output[-PREDICTION_LENGTH:], targets[-PREDICTION_LENGTH:]).cpu().item()            \n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edvard/.pyenv/versions/3.10.7/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:381: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     9/   46 batches | lr 0.005000 | 16.78 ms | loss 163.68266 |\n",
      "| epoch   1 |    18/   46 batches | lr 0.005000 | 13.74 ms | loss 187.87901 |\n",
      "| epoch   1 |    27/   46 batches | lr 0.005000 | 13.17 ms | loss 157.08022 |\n",
      "| epoch   1 |    36/   46 batches | lr 0.005000 | 13.46 ms | loss 138.06402 |\n",
      "| epoch   1 |    45/   46 batches | lr 0.005000 | 13.20 ms | loss 153.38615 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.67s | valid loss 80.33483 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |     9/   46 batches | lr 0.004513 | 14.27 ms | loss 134.22037 |\n",
      "| epoch   2 |    18/   46 batches | lr 0.004513 | 13.12 ms | loss 132.65797 |\n",
      "| epoch   2 |    27/   46 batches | lr 0.004513 | 13.97 ms | loss 108.32177 |\n",
      "| epoch   2 |    36/   46 batches | lr 0.004513 | 13.42 ms | loss 91.36860 |\n",
      "| epoch   2 |    45/   46 batches | lr 0.004513 | 14.19 ms | loss 99.14156 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.66s | valid loss 40.98504 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |     9/   46 batches | lr 0.004287 | 20.59 ms | loss 108.45089 |\n",
      "| epoch   3 |    18/   46 batches | lr 0.004287 | 13.16 ms | loss 86.12490 |\n",
      "| epoch   3 |    27/   46 batches | lr 0.004287 | 13.32 ms | loss 74.97731 |\n",
      "| epoch   3 |    36/   46 batches | lr 0.004287 | 12.96 ms | loss 59.29951 |\n",
      "| epoch   3 |    45/   46 batches | lr 0.004287 | 12.89 ms | loss 69.53400 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.69s | valid loss 24.34228 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |     9/   46 batches | lr 0.004073 | 13.99 ms | loss 59.28803 |\n",
      "| epoch   4 |    18/   46 batches | lr 0.004073 | 12.70 ms | loss 42.46456 |\n",
      "| epoch   4 |    27/   46 batches | lr 0.004073 | 12.83 ms | loss 58.77019 |\n",
      "| epoch   4 |    36/   46 batches | lr 0.004073 | 12.69 ms | loss 40.24655 |\n",
      "| epoch   4 |    45/   46 batches | lr 0.004073 | 12.86 ms | loss 34.69241 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.62s | valid loss 13.55017 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |     9/   46 batches | lr 0.003869 | 14.31 ms | loss 46.32511 |\n",
      "| epoch   5 |    18/   46 batches | lr 0.003869 | 12.74 ms | loss 35.90355 |\n",
      "| epoch   5 |    27/   46 batches | lr 0.003869 | 12.75 ms | loss 36.51816 |\n",
      "| epoch   5 |    36/   46 batches | lr 0.003869 | 12.75 ms | loss 33.89644 |\n",
      "| epoch   5 |    45/   46 batches | lr 0.003869 | 13.54 ms | loss 24.78325 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.63s | valid loss 7.76187 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |     9/   46 batches | lr 0.003675 | 14.03 ms | loss 28.18352 |\n",
      "| epoch   6 |    18/   46 batches | lr 0.003675 | 12.74 ms | loss 15.59815 |\n",
      "| epoch   6 |    27/   46 batches | lr 0.003675 | 12.73 ms | loss 26.00513 |\n",
      "| epoch   6 |    36/   46 batches | lr 0.003675 | 12.70 ms | loss 28.21173 |\n",
      "| epoch   6 |    45/   46 batches | lr 0.003675 | 12.47 ms | loss 12.46023 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.61s | valid loss 4.90005 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |     9/   46 batches | lr 0.003492 | 13.98 ms | loss 12.06007 |\n",
      "| epoch   7 |    18/   46 batches | lr 0.003492 | 12.67 ms | loss 15.72165 |\n",
      "| epoch   7 |    27/   46 batches | lr 0.003492 | 12.79 ms | loss 20.61246 |\n",
      "| epoch   7 |    36/   46 batches | lr 0.003492 | 12.62 ms | loss 13.87028 |\n",
      "| epoch   7 |    45/   46 batches | lr 0.003492 | 12.59 ms | loss 14.37386 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.61s | valid loss 4.73452 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |     9/   46 batches | lr 0.003317 | 13.85 ms | loss 20.29246 |\n",
      "| epoch   8 |    18/   46 batches | lr 0.003317 | 12.83 ms | loss 19.23095 |\n",
      "| epoch   8 |    27/   46 batches | lr 0.003317 | 12.54 ms | loss 11.78598 |\n",
      "| epoch   8 |    36/   46 batches | lr 0.003317 | 12.65 ms | loss 14.45192 |\n",
      "| epoch   8 |    45/   46 batches | lr 0.003317 | 12.46 ms | loss 15.51888 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.61s | valid loss 4.15705 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |     9/   46 batches | lr 0.003151 | 14.13 ms | loss 18.86528 |\n",
      "| epoch   9 |    18/   46 batches | lr 0.003151 | 12.69 ms | loss 12.76635 |\n",
      "| epoch   9 |    27/   46 batches | lr 0.003151 | 12.67 ms | loss 12.07379 |\n",
      "| epoch   9 |    36/   46 batches | lr 0.003151 | 12.80 ms | loss 10.34363 |\n",
      "| epoch   9 |    45/   46 batches | lr 0.003151 | 12.57 ms | loss 7.65539 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.62s | valid loss 2.91264 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |     9/   46 batches | lr 0.002994 | 13.99 ms | loss 9.86160 |\n",
      "| epoch  10 |    18/   46 batches | lr 0.002994 | 12.79 ms | loss 9.06463 |\n",
      "| epoch  10 |    27/   46 batches | lr 0.002994 | 12.66 ms | loss 9.44912 |\n",
      "| epoch  10 |    36/   46 batches | lr 0.002994 | 12.86 ms | loss 16.67068 |\n",
      "| epoch  10 |    45/   46 batches | lr 0.002994 | 12.67 ms | loss 12.23131 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  0.62s | valid loss 3.52882 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |     9/   46 batches | lr 0.002844 | 14.27 ms | loss 6.27068 |\n",
      "| epoch  11 |    18/   46 batches | lr 0.002844 | 12.85 ms | loss 15.82201 |\n",
      "| epoch  11 |    27/   46 batches | lr 0.002844 | 12.76 ms | loss 5.65827 |\n",
      "| epoch  11 |    36/   46 batches | lr 0.002844 | 12.65 ms | loss 10.28036 |\n",
      "| epoch  11 |    45/   46 batches | lr 0.002844 | 12.73 ms | loss 11.05458 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  0.62s | valid loss 2.44146 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |     9/   46 batches | lr 0.002702 | 14.06 ms | loss 8.20972 |\n",
      "| epoch  12 |    18/   46 batches | lr 0.002702 | 12.78 ms | loss 7.44264 |\n",
      "| epoch  12 |    27/   46 batches | lr 0.002702 | 12.74 ms | loss 10.59028 |\n",
      "| epoch  12 |    36/   46 batches | lr 0.002702 | 12.71 ms | loss 8.43182 |\n",
      "| epoch  12 |    45/   46 batches | lr 0.002702 | 12.67 ms | loss 11.52952 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  0.62s | valid loss 1.30735 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |     9/   46 batches | lr 0.002567 | 14.36 ms | loss 7.13255 |\n",
      "| epoch  13 |    18/   46 batches | lr 0.002567 | 12.88 ms | loss 11.79327 |\n",
      "| epoch  13 |    27/   46 batches | lr 0.002567 | 12.72 ms | loss 5.58102 |\n",
      "| epoch  13 |    36/   46 batches | lr 0.002567 | 12.95 ms | loss 7.53762 |\n",
      "| epoch  13 |    45/   46 batches | lr 0.002567 | 12.73 ms | loss 7.30819 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  0.62s | valid loss 1.19739 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |     9/   46 batches | lr 0.002438 | 14.26 ms | loss 9.97427 |\n",
      "| epoch  14 |    18/   46 batches | lr 0.002438 | 12.78 ms | loss 8.16501 |\n",
      "| epoch  14 |    27/   46 batches | lr 0.002438 | 12.60 ms | loss 6.82536 |\n",
      "| epoch  14 |    36/   46 batches | lr 0.002438 | 12.90 ms | loss 9.57890 |\n",
      "| epoch  14 |    45/   46 batches | lr 0.002438 | 13.13 ms | loss 4.44055 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  0.62s | valid loss 1.49111 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |     9/   46 batches | lr 0.002316 | 14.31 ms | loss 8.32668 |\n",
      "| epoch  15 |    18/   46 batches | lr 0.002316 | 12.83 ms | loss 8.27858 |\n",
      "| epoch  15 |    27/   46 batches | lr 0.002316 | 12.80 ms | loss 8.95704 |\n",
      "| epoch  15 |    36/   46 batches | lr 0.002316 | 12.71 ms | loss 5.64325 |\n",
      "| epoch  15 |    45/   46 batches | lr 0.002316 | 12.64 ms | loss 5.26287 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  0.62s | valid loss 2.50880 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |     9/   46 batches | lr 0.002201 | 14.19 ms | loss 5.54500 |\n",
      "| epoch  16 |    18/   46 batches | lr 0.002201 | 12.84 ms | loss 7.28288 |\n",
      "| epoch  16 |    27/   46 batches | lr 0.002201 | 12.81 ms | loss 7.36212 |\n",
      "| epoch  16 |    36/   46 batches | lr 0.002201 | 12.57 ms | loss 6.87189 |\n",
      "| epoch  16 |    45/   46 batches | lr 0.002201 | 12.76 ms | loss 5.93737 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  0.62s | valid loss 1.90013 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |     9/   46 batches | lr 0.002091 | 14.41 ms | loss 5.38282 |\n",
      "| epoch  17 |    18/   46 batches | lr 0.002091 | 12.38 ms | loss 5.02250 |\n",
      "| epoch  17 |    27/   46 batches | lr 0.002091 | 12.83 ms | loss 7.85186 |\n",
      "| epoch  17 |    36/   46 batches | lr 0.002091 | 12.74 ms | loss 6.55963 |\n",
      "| epoch  17 |    45/   46 batches | lr 0.002091 | 12.88 ms | loss 4.05781 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  0.62s | valid loss 2.05666 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |     9/   46 batches | lr 0.001986 | 14.00 ms | loss 3.51642 |\n",
      "| epoch  18 |    18/   46 batches | lr 0.001986 | 12.94 ms | loss 4.63200 |\n",
      "| epoch  18 |    27/   46 batches | lr 0.001986 | 12.97 ms | loss 5.14071 |\n",
      "| epoch  18 |    36/   46 batches | lr 0.001986 | 12.87 ms | loss 4.09856 |\n",
      "| epoch  18 |    45/   46 batches | lr 0.001986 | 12.73 ms | loss 5.03716 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  0.62s | valid loss 2.26033 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |     9/   46 batches | lr 0.001887 | 14.07 ms | loss 4.98911 |\n",
      "| epoch  19 |    18/   46 batches | lr 0.001887 | 12.69 ms | loss 5.89485 |\n",
      "| epoch  19 |    27/   46 batches | lr 0.001887 | 12.78 ms | loss 5.63219 |\n",
      "| epoch  19 |    36/   46 batches | lr 0.001887 | 12.72 ms | loss 6.04337 |\n",
      "| epoch  19 |    45/   46 batches | lr 0.001887 | 12.48 ms | loss 5.57915 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  0.61s | valid loss 2.97653 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |     9/   46 batches | lr 0.001792 | 14.05 ms | loss 6.56368 |\n",
      "| epoch  20 |    18/   46 batches | lr 0.001792 | 12.80 ms | loss 3.52480 |\n",
      "| epoch  20 |    27/   46 batches | lr 0.001792 | 12.80 ms | loss 3.86510 |\n",
      "| epoch  20 |    36/   46 batches | lr 0.001792 | 12.90 ms | loss 4.06541 |\n",
      "| epoch  20 |    45/   46 batches | lr 0.001792 | 12.70 ms | loss 2.98207 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  0.62s | valid loss 1.24174 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |     9/   46 batches | lr 0.001703 | 14.20 ms | loss 4.60328 |\n",
      "| epoch  21 |    18/   46 batches | lr 0.001703 | 12.77 ms | loss 3.67964 |\n",
      "| epoch  21 |    27/   46 batches | lr 0.001703 | 12.79 ms | loss 4.25651 |\n",
      "| epoch  21 |    36/   46 batches | lr 0.001703 | 12.74 ms | loss 3.92340 |\n",
      "| epoch  21 |    45/   46 batches | lr 0.001703 | 12.64 ms | loss 4.93010 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  0.62s | valid loss 2.53544 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |     9/   46 batches | lr 0.001618 | 14.37 ms | loss 2.04818 |\n",
      "| epoch  22 |    18/   46 batches | lr 0.001618 | 12.98 ms | loss 5.08199 |\n",
      "| epoch  22 |    27/   46 batches | lr 0.001618 | 12.84 ms | loss 3.85313 |\n",
      "| epoch  22 |    36/   46 batches | lr 0.001618 | 12.88 ms | loss 2.26975 |\n",
      "| epoch  22 |    45/   46 batches | lr 0.001618 | 12.84 ms | loss 3.69322 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  0.63s | valid loss 3.49059 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |     9/   46 batches | lr 0.001537 | 14.24 ms | loss 3.54682 |\n",
      "| epoch  23 |    18/   46 batches | lr 0.001537 | 12.66 ms | loss 3.65518 |\n",
      "| epoch  23 |    27/   46 batches | lr 0.001537 | 13.12 ms | loss 3.73873 |\n",
      "| epoch  23 |    36/   46 batches | lr 0.001537 | 12.69 ms | loss 2.90648 |\n",
      "| epoch  23 |    45/   46 batches | lr 0.001537 | 13.11 ms | loss 5.39252 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  0.63s | valid loss 3.46031 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |     9/   46 batches | lr 0.001460 | 14.37 ms | loss 4.11302 |\n",
      "| epoch  24 |    18/   46 batches | lr 0.001460 | 13.02 ms | loss 2.96397 |\n",
      "| epoch  24 |    27/   46 batches | lr 0.001460 | 13.38 ms | loss 4.00484 |\n",
      "| epoch  24 |    36/   46 batches | lr 0.001460 | 12.94 ms | loss 2.94143 |\n",
      "| epoch  24 |    45/   46 batches | lr 0.001460 | 12.79 ms | loss 3.26574 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  0.63s | valid loss 2.37834 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |     9/   46 batches | lr 0.001387 | 14.21 ms | loss 3.04521 |\n",
      "| epoch  25 |    18/   46 batches | lr 0.001387 | 12.65 ms | loss 2.53210 |\n",
      "| epoch  25 |    27/   46 batches | lr 0.001387 | 12.79 ms | loss 2.97027 |\n",
      "| epoch  25 |    36/   46 batches | lr 0.001387 | 12.94 ms | loss 4.15177 |\n",
      "| epoch  25 |    45/   46 batches | lr 0.001387 | 13.18 ms | loss 4.22559 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  0.63s | valid loss 1.72600 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |     9/   46 batches | lr 0.001318 | 14.44 ms | loss 3.54696 |\n",
      "| epoch  26 |    18/   46 batches | lr 0.001318 | 12.99 ms | loss 2.01271 |\n",
      "| epoch  26 |    27/   46 batches | lr 0.001318 | 13.08 ms | loss 2.54152 |\n",
      "| epoch  26 |    36/   46 batches | lr 0.001318 | 13.02 ms | loss 2.86514 |\n",
      "| epoch  26 |    45/   46 batches | lr 0.001318 | 12.94 ms | loss 2.65402 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  0.63s | valid loss 1.48597 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |     9/   46 batches | lr 0.001252 | 14.59 ms | loss 1.93232 |\n",
      "| epoch  27 |    18/   46 batches | lr 0.001252 | 12.80 ms | loss 3.61715 |\n",
      "| epoch  27 |    27/   46 batches | lr 0.001252 | 13.48 ms | loss 4.30406 |\n",
      "| epoch  27 |    36/   46 batches | lr 0.001252 | 12.87 ms | loss 2.38239 |\n",
      "| epoch  27 |    45/   46 batches | lr 0.001252 | 12.86 ms | loss 1.50556 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  0.63s | valid loss 1.90507 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |     9/   46 batches | lr 0.001189 | 14.29 ms | loss 4.65424 |\n",
      "| epoch  28 |    18/   46 batches | lr 0.001189 | 12.96 ms | loss 1.49111 |\n",
      "| epoch  28 |    27/   46 batches | lr 0.001189 | 12.97 ms | loss 3.17983 |\n",
      "| epoch  28 |    36/   46 batches | lr 0.001189 | 12.92 ms | loss 5.32880 |\n",
      "| epoch  28 |    45/   46 batches | lr 0.001189 | 12.67 ms | loss 3.60587 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  0.63s | valid loss 2.35035 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |     9/   46 batches | lr 0.001130 | 14.24 ms | loss 3.59036 |\n",
      "| epoch  29 |    18/   46 batches | lr 0.001130 | 12.98 ms | loss 4.81328 |\n",
      "| epoch  29 |    27/   46 batches | lr 0.001130 | 13.19 ms | loss 3.67809 |\n",
      "| epoch  29 |    36/   46 batches | lr 0.001130 | 12.98 ms | loss 2.93501 |\n",
      "| epoch  29 |    45/   46 batches | lr 0.001130 | 13.22 ms | loss 2.45904 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  0.63s | valid loss 3.63269 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |     9/   46 batches | lr 0.001073 | 14.22 ms | loss 2.30334 |\n",
      "| epoch  30 |    18/   46 batches | lr 0.001073 | 13.05 ms | loss 3.47137 |\n",
      "| epoch  30 |    27/   46 batches | lr 0.001073 | 12.83 ms | loss 2.63244 |\n",
      "| epoch  30 |    36/   46 batches | lr 0.001073 | 13.09 ms | loss 2.65543 |\n",
      "| epoch  30 |    45/   46 batches | lr 0.001073 | 12.76 ms | loss 4.30079 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  0.63s | valid loss 2.54070 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |     9/   46 batches | lr 0.001020 | 14.45 ms | loss 2.10094 |\n",
      "| epoch  31 |    18/   46 batches | lr 0.001020 | 13.02 ms | loss 1.32074 |\n",
      "| epoch  31 |    27/   46 batches | lr 0.001020 | 12.80 ms | loss 2.56409 |\n",
      "| epoch  31 |    36/   46 batches | lr 0.001020 | 12.97 ms | loss 1.90746 |\n",
      "| epoch  31 |    45/   46 batches | lr 0.001020 | 12.88 ms | loss 2.95471 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  0.63s | valid loss 4.33722 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |     9/   46 batches | lr 0.000969 | 13.90 ms | loss 3.69499 |\n",
      "| epoch  32 |    18/   46 batches | lr 0.000969 | 13.12 ms | loss 1.55748 |\n",
      "| epoch  32 |    27/   46 batches | lr 0.000969 | 13.15 ms | loss 2.73768 |\n",
      "| epoch  32 |    36/   46 batches | lr 0.000969 | 13.10 ms | loss 1.87294 |\n",
      "| epoch  32 |    45/   46 batches | lr 0.000969 | 13.03 ms | loss 5.45039 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  0.63s | valid loss 3.08329 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |     9/   46 batches | lr 0.000920 | 14.33 ms | loss 2.85469 |\n",
      "| epoch  33 |    18/   46 batches | lr 0.000920 | 13.18 ms | loss 2.00634 |\n",
      "| epoch  33 |    27/   46 batches | lr 0.000920 | 13.45 ms | loss 2.01249 |\n",
      "| epoch  33 |    36/   46 batches | lr 0.000920 | 12.84 ms | loss 2.06084 |\n",
      "| epoch  33 |    45/   46 batches | lr 0.000920 | 13.29 ms | loss 2.59449 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  0.64s | valid loss 3.85606 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |     9/   46 batches | lr 0.000874 | 14.22 ms | loss 2.73516 |\n",
      "| epoch  34 |    18/   46 batches | lr 0.000874 | 12.96 ms | loss 1.55158 |\n",
      "| epoch  34 |    27/   46 batches | lr 0.000874 | 12.86 ms | loss 2.57927 |\n",
      "| epoch  34 |    36/   46 batches | lr 0.000874 | 13.11 ms | loss 2.92912 |\n",
      "| epoch  34 |    45/   46 batches | lr 0.000874 | 12.95 ms | loss 2.30001 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  0.63s | valid loss 1.97970 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |     9/   46 batches | lr 0.000830 | 14.27 ms | loss 2.13669 |\n",
      "| epoch  35 |    18/   46 batches | lr 0.000830 | 12.99 ms | loss 3.76937 |\n",
      "| epoch  35 |    27/   46 batches | lr 0.000830 | 13.15 ms | loss 1.96761 |\n",
      "| epoch  35 |    36/   46 batches | lr 0.000830 | 12.80 ms | loss 1.82703 |\n",
      "| epoch  35 |    45/   46 batches | lr 0.000830 | 12.95 ms | loss 2.28182 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  0.63s | valid loss 3.86276 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |     9/   46 batches | lr 0.000789 | 14.26 ms | loss 2.63631 |\n",
      "| epoch  36 |    18/   46 batches | lr 0.000789 | 12.91 ms | loss 1.94438 |\n",
      "| epoch  36 |    27/   46 batches | lr 0.000789 | 13.21 ms | loss 1.37157 |\n",
      "| epoch  36 |    36/   46 batches | lr 0.000789 | 12.97 ms | loss 2.06766 |\n",
      "| epoch  36 |    45/   46 batches | lr 0.000789 | 12.99 ms | loss 1.61433 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  0.63s | valid loss 4.62979 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |     9/   46 batches | lr 0.000749 | 14.30 ms | loss 1.77693 |\n",
      "| epoch  37 |    18/   46 batches | lr 0.000749 | 13.08 ms | loss 2.14915 |\n",
      "| epoch  37 |    27/   46 batches | lr 0.000749 | 13.12 ms | loss 1.53817 |\n",
      "| epoch  37 |    36/   46 batches | lr 0.000749 | 13.08 ms | loss 1.58922 |\n",
      "| epoch  37 |    45/   46 batches | lr 0.000749 | 13.15 ms | loss 2.12001 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  0.63s | valid loss 2.71410 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |     9/   46 batches | lr 0.000712 | 14.64 ms | loss 2.05851 |\n",
      "| epoch  38 |    18/   46 batches | lr 0.000712 | 13.35 ms | loss 2.24021 |\n",
      "| epoch  38 |    27/   46 batches | lr 0.000712 | 12.94 ms | loss 1.36370 |\n",
      "| epoch  38 |    36/   46 batches | lr 0.000712 | 12.64 ms | loss 1.91770 |\n",
      "| epoch  38 |    45/   46 batches | lr 0.000712 | 13.11 ms | loss 1.91457 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  0.63s | valid loss 2.50641 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |     9/   46 batches | lr 0.000676 | 14.40 ms | loss 3.08212 |\n",
      "| epoch  39 |    18/   46 batches | lr 0.000676 | 13.01 ms | loss 3.06113 |\n",
      "| epoch  39 |    27/   46 batches | lr 0.000676 | 12.88 ms | loss 2.74862 |\n",
      "| epoch  39 |    36/   46 batches | lr 0.000676 | 12.89 ms | loss 1.64488 |\n",
      "| epoch  39 |    45/   46 batches | lr 0.000676 | 13.05 ms | loss 2.44165 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  0.63s | valid loss 2.34044 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |     9/   46 batches | lr 0.000643 | 14.40 ms | loss 2.79805 |\n",
      "| epoch  40 |    18/   46 batches | lr 0.000643 | 12.82 ms | loss 2.20966 |\n",
      "| epoch  40 |    27/   46 batches | lr 0.000643 | 13.00 ms | loss 1.77167 |\n",
      "| epoch  40 |    36/   46 batches | lr 0.000643 | 12.81 ms | loss 1.86883 |\n",
      "| epoch  40 |    45/   46 batches | lr 0.000643 | 12.78 ms | loss 1.92372 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time:  0.63s | valid loss 2.20777 |\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_data)\n",
    "    \n",
    "    \n",
    "    # if(epoch % 10 == 0):\n",
    "    #     val_loss = plot_and_loss(model, val_data,epoch)\n",
    "    #     predict_future(model, val_data,200)\n",
    "    # else:\n",
    "    val_loss = evaluate(model, test_data)\n",
    "        \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} |'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss))\n",
    "    print('-' * 89)\n",
    "\n",
    "    # #if val_loss < best_val_loss:\n",
    "    # #    best_val_loss = val_loss\n",
    "    # #    best_model = model\n",
    "\n",
    "    scheduler.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        seq_len = data.size(0)\n",
    "        if seq_len != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:seq_len, :seq_len]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4d8082ac81b5767605eb477108b3a93415a0db0c81fcb1a05e0921a30ce1269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
